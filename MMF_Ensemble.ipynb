{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsmeeeeeee/MML/blob/main/MMF_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxtVTjH4DCCV"
      },
      "source": [
        "\n",
        "#  Ensemble Method on (MFA) Multimodal Fusion (ohne Self-Attention) for Multimodal Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAAp9bSj3GIb"
      },
      "source": [
        "Group: 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OtrWWSWN4kbD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb45d8d-43b9-49c4-a405-9f1c8d8b35cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install torch torchvision\n",
        "! pip install transformers pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YIBVh3Ie3j3k"
      },
      "outputs": [],
      "source": [
        "from torchvision import models, transforms\n",
        "from PIL import Image, ImageFile\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhuU0bog5CxN",
        "outputId": "23ca1f12-4999-495f-db23-883357a86028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AQYwsMC1aP-E"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tmHlbvkD--p"
      },
      "source": [
        "**Data extraction is made with Resnet and Bert by Features_extraction file:** https://colab.research.google.com/drive/144qkv0HiAqRXuwlfGNi9M-DxNGmBffYV?usp=drive_link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Ba7Vpc_C2m"
      },
      "source": [
        "### **Read the data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Load labels from CSV\n",
        "file_path_labels = \"/content/drive/MyDrive/MultimodalNLP/projekt/data/labels.csv\"\n",
        "labeled_data = pd.read_csv(file_path_labels)\n",
        "labels = labeled_data['overall_sentiment'].values\n",
        "\n",
        "# Define your existing mapping\n",
        "label_mapping = {\n",
        "    \"very_negative\": 0,\n",
        "    \"negative\": 0,\n",
        "    \"positive\": 1,\n",
        "    \"very_positive\": 1\n",
        "}\n",
        "\n",
        "# Get valid indices for labels that exist in the mapping\n",
        "valid_indices = [i for i, label in enumerate(labels) if label in label_mapping]\n",
        "filtered_labels = [labels[i] for i in valid_indices]\n",
        "\n",
        "# Apply the mapping to convert filtered text labels to numeric labels\n",
        "numeric_labels = np.array([label_mapping[label] for label in filtered_labels])\n",
        "\n",
        "# Convert labels to a torch tensor\n",
        "numeric_labels = torch.tensor(numeric_labels, dtype=torch.long)\n",
        "print(\"numeric labels:\", numeric_labels.shape)\n",
        "\n",
        "# Load image features\n",
        "file_path_im = \"/content/drive/MyDrive/MultimodalNLP/projekt/features_data/image_features_restnet.npy\"\n",
        "image_features = np.load(file_path_im)\n",
        "\n",
        "# Load text features\n",
        "file_path_emb = \"/content/drive/MyDrive/MultimodalNLP/projekt/features_data/text_features_bert.npy\"\n",
        "text_features = np.load(file_path_emb)\n",
        "\n",
        "# Filter image and text features using valid indices\n",
        "filtered_image_features = torch.tensor(image_features[valid_indices], dtype=torch.float32)\n",
        "filtered_text_features = torch.tensor(text_features[valid_indices], dtype=torch.float32)\n",
        "\n",
        "print(\"Filtered Image features:\", filtered_image_features.shape)\n",
        "print(\"Filtered Text features:\", filtered_text_features.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_r0i9Tc2mTA",
        "outputId": "87e0bdc9-a72c-4fed-bfb8-3a78540d8418"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numeric labels: torch.Size([4791])\n",
            "Filtered Image features: torch.Size([4791, 1000])\n",
            "Filtered Text features: torch.Size([4791, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t_GnV9ZGY1s"
      },
      "source": [
        "### **Split the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "clcSK7I3F5qL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train_text_bert, test_text_bert, train_image_restnet, test_image_restnet, train_labels, test_labels = train_test_split(filtered_text_features,\n",
        "    filtered_image_features, numeric_labels, test_size=0.2, random_state=42)  #20%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DHDjANnymThP",
        "outputId": "fcb1965e-98a6-404d-88d2-b44f73f2baa5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3832"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(train_text_bert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DQeLJ7d1fX-j"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oHDO1j7wjqF-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert in Tendors\n",
        "\n",
        "\n",
        "train_dataset_bert_restnet = TensorDataset(train_text_bert, train_image_restnet, train_labels)\n",
        "\n",
        "test_dataset_bert_restnet = TensorDataset(test_text_bert, test_image_restnet, test_labels)\n",
        "\n",
        "test_loader = DataLoader(test_dataset_bert_restnet, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbNKCkegDy6O"
      },
      "source": [
        "\n",
        "\n",
        "## **Build the Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERsth2odfHgN"
      },
      "source": [
        "### **LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "94Dh1nWqfjOo"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # For bidirectional LSTM\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden and cell states\n",
        "        batch_size = x.size(0)\n",
        "        num_directions = 2 if self.lstm.bidirectional else 1\n",
        "        h0 = torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.lstm.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])  # Use the last timestep\n",
        "        return out\n",
        "\n",
        "class MultimodalFusionLSTMClassifier(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, hidden_dim, lstm_hidden_dim, num_classes):\n",
        "        super(MultimodalFusionLSTMClassifier, self).__init__()\n",
        "        self.text_model = nn.Linear(text_dim, hidden_dim)\n",
        "        self.image_model = nn.Linear(image_dim, hidden_dim)\n",
        "        # LSTM Classifier directly uses combined features\n",
        "        self.classifier = LSTMClassifier(hidden_dim * 2, lstm_hidden_dim, num_layers=2, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "        # Process text and image features through linear layers\n",
        "        text_features = self.text_model(text_features)\n",
        "        image_features = self.image_model(image_features)\n",
        "\n",
        "        # Combine text and image features\n",
        "        combined_features = torch.cat([text_features, image_features], dim=1)\n",
        "        combined_features = combined_features.unsqueeze(1)  # Adding sequence dimension for LSTM\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(combined_features)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7u9zqu8kp5u",
        "outputId": "c73fd39a-97d0-465f-8cfd-9035d00bbc79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: Best Validation Loss: 0.4248\n",
            "Fold 1: Best Validation Loss: 0.3901\n",
            "Fold 2: Best Validation Loss: 0.3967\n",
            "Fold 3: Best Validation Loss: 0.4229\n",
            "Fold 4: Best Validation Loss: 0.4233\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    \"\"\"Evaluate the model on given data loader and return the average loss.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for text_features, image_features, labels in loader:\n",
        "            text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "            outputs = model(text_features, image_features)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def train_and_save_best_models_lstm(train_dataset, num_folds=5, num_epochs=10):\n",
        "    kf = KFold(n_splits=num_folds)\n",
        "    best_models = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        val_subset = Subset(train_dataset, val_idx)\n",
        "        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        dev_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
        "\n",
        "        model = MultimodalFusionLSTMClassifier(text_dim=768, image_dim=1000, hidden_dim=128, lstm_hidden_dim=256, num_classes=2).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for text_features, image_features, labels in train_loader:\n",
        "                text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(text_features, image_features)\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Evaluate on the development set\n",
        "        dev_loss = evaluate_model(model, dev_loader, device)\n",
        "        if dev_loss < best_val_loss:\n",
        "            best_val_loss = dev_loss\n",
        "            best_model_state = model\n",
        "\n",
        "\n",
        "\n",
        "        best_models.append(best_model_state)\n",
        "        torch.save(best_model_state, f'best_model_fold_{fold}.pth')\n",
        "        print(f\"Fold {fold}: Best Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return best_models\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "best_models_lstm = train_and_save_best_models_lstm(train_dataset_bert_restnet,num_folds=5, num_epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdw0A_d7f8w2"
      },
      "source": [
        "###**RNN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Classifier_RNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super(Classifier_RNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=False)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)  # Unidirectional RNN\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize the hidden state\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.rnn.num_layers, batch_size, self.rnn.hidden_size).to(x.device)\n",
        "        # Process inputs through the RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        # Use the output from the last timestep\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "class MultimodalFusionRNNClassifier(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, hidden_dim, rnn_hidden_dim, num_classes):\n",
        "        super(MultimodalFusionRNNClassifier, self).__init__()\n",
        "        self.text_model = nn.Linear(text_dim, hidden_dim)\n",
        "        self.image_model = nn.Linear(image_dim, hidden_dim)\n",
        "        # RNN Classifier\n",
        "        self.classifier = Classifier_RNN(hidden_dim * 2, rnn_hidden_dim, num_layers=2, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "        # Process text and image features directly through their respective models\n",
        "        text_features = self.text_model(text_features)\n",
        "        image_features = self.image_model(image_features)\n",
        "        # Combine the text and image features\n",
        "        combined_features = torch.cat([text_features, image_features], dim=1)\n",
        "        combined_features = combined_features.unsqueeze(1)  # Adding a sequence dimension for RNN\n",
        "        # Classification\n",
        "        output = self.classifier(combined_features)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "Gk6gAC9xKmit"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn4j5TGrk7og",
        "outputId": "b6843eb3-1619-475d-b8a6-a22def7206ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: Best Validation Loss: 0.4207\n",
            "Fold 1: Best Validation Loss: 0.4062\n",
            "Fold 2: Best Validation Loss: 0.3981\n",
            "Fold 3: Best Validation Loss: 0.4333\n",
            "Fold 4: Best Validation Loss: 0.4149\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    \"\"\"Evaluate the model on given data loader and return the average loss.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for text_features, image_features, labels in loader:\n",
        "            text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "            outputs = model(text_features, image_features)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def train_and_save_best_models_rnn(train_dataset, num_folds=5, num_epochs=10):\n",
        "    kf = KFold(n_splits=num_folds)\n",
        "    best_models = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        val_subset = Subset(train_dataset, val_idx)\n",
        "        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        dev_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
        "\n",
        "        model = MultimodalFusionRNNClassifier(text_dim=768, image_dim=1000, hidden_dim=128, rnn_hidden_dim=128, num_classes=2).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for text_features, image_features, labels in train_loader:\n",
        "                text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(text_features, image_features)\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Evaluate on the development set\n",
        "        dev_loss = evaluate_model(model, dev_loader, device)\n",
        "        if dev_loss < best_val_loss:\n",
        "            best_val_loss = dev_loss\n",
        "            best_model_state = copy.deepcopy(model)\n",
        "\n",
        "\n",
        "\n",
        "        best_models.append(best_model_state)\n",
        "        torch.save(best_model_state, f'best_model_fold_{fold}.pth')\n",
        "        print(f\"Fold {fold}: Best Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return best_models\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "best_models_rrn = train_and_save_best_models_rnn(train_dataset_bert_restnet,num_folds=5, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imn54lR3gELd"
      },
      "source": [
        "###**CNN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, input_channels, num_channels, num_classes, hidden_dim):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        kernel_size = 3\n",
        "        padding = 1\n",
        "        stride = 1\n",
        "        pool_kernel_size = 2\n",
        "        pool_stride = 2\n",
        "\n",
        "        # Calculate the size after each layer\n",
        "        size_after_conv = (hidden_dim + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
        "        size_after_pool = size_after_conv // pool_stride\n",
        "\n",
        "        size_after_conv2 = (size_after_pool + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
        "        final_size = size_after_conv2 // pool_stride\n",
        "\n",
        "        self.conv1 = nn.Conv1d(input_channels, num_channels, kernel_size, padding=padding)\n",
        "        self.conv2 = nn.Conv1d(num_channels, num_channels * 2, kernel_size, padding=padding)\n",
        "        self.pool = nn.MaxPool1d(pool_kernel_size, stride=pool_stride)\n",
        "\n",
        "        # Fully connected layer input size calculation\n",
        "        self.fc_input_size = num_channels * 2 * final_size\n",
        "        self.fc = nn.Linear(self.fc_input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class MultimodalFusionCNNClassifier(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, hidden_dim, num_classes):\n",
        "        super(MultimodalFusionCNNClassifier, self).__init__()\n",
        "        self.text_model = nn.Linear(text_dim, hidden_dim)\n",
        "        self.image_model = nn.Linear(image_dim, hidden_dim)\n",
        "        self.classifier = CNNClassifier(2, hidden_dim, num_classes, hidden_dim)\n",
        "        self.hidden_dim = hidden_dim  # Ensure hidden_dim is stored as an instance variable\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "        text_features = self.text_model(text_features)\n",
        "        image_features = self.image_model(image_features)\n",
        "        # Combine text and image features\n",
        "        combined_features = torch.cat([text_features, image_features], dim=1)\n",
        "        combined_features = combined_features.view(-1, 2, self.hidden_dim)  # Use self.hidden_dim correctly\n",
        "        # Classification\n",
        "        output = self.classifier(combined_features)\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "B2xYQB92QRxu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvRr_ZVxlEjx",
        "outputId": "5b5922c3-dfa3-44a2-e6f3-c872b058f1a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: Best Validation Loss: 0.4878\n",
            "Fold 1: Best Validation Loss: 0.4358\n",
            "Fold 2: Best Validation Loss: 0.4179\n",
            "Fold 3: Best Validation Loss: 0.5207\n",
            "Fold 4: Best Validation Loss: 0.5260\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    \"\"\"Evaluate the model on given data loader and return the average loss.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for text_features, image_features, labels in loader:\n",
        "            text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "            outputs = model(text_features, image_features)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def train_and_save_best_models_cnn(train_dataset, num_folds=5, num_epochs=10):\n",
        "    kf = KFold(n_splits=num_folds)\n",
        "    best_models = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        val_subset = Subset(train_dataset, val_idx)\n",
        "        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        dev_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
        "\n",
        "        model = MultimodalFusionCNNClassifier(text_dim=768, image_dim=1000, hidden_dim=128, num_classes=2).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for text_features, image_features, labels in train_loader:\n",
        "                text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(text_features, image_features)\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Evaluate on the development set\n",
        "        dev_loss = evaluate_model(model, dev_loader, device)\n",
        "        if dev_loss < best_val_loss:\n",
        "            best_val_loss = dev_loss\n",
        "            best_model_state = copy.deepcopy(model)\n",
        "\n",
        "\n",
        "\n",
        "        best_models.append(best_model_state)\n",
        "        torch.save(best_model_state, f'best_model_fold_{fold}.pth')\n",
        "        print(f\"Fold {fold}: Best Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return best_models\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "best_models_crn = train_and_save_best_models_cnn(train_dataset_bert_restnet,num_folds=5, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39vgMIINDLo8"
      },
      "source": [
        "### **Simple model with softmax**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleFusionClassifier(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, hidden_dim, num_classes):\n",
        "        super(SimpleFusionClassifier, self).__init__()\n",
        "        self.text_model = nn.Linear(text_dim, hidden_dim)\n",
        "        self.image_model = nn.Linear(image_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "        # Process text and image features through their respective linear transformations\n",
        "        text_features = self.text_model(text_features)\n",
        "        image_features = self.image_model(image_features)\n",
        "\n",
        "        # Concatenate features from both modalities\n",
        "        combined_features = torch.cat([text_features, image_features], dim=1)\n",
        "\n",
        "        # Pass the combined features through the final fully connected layer\n",
        "        # The outputs are logits and should not be passed through softmax here when using CrossEntropyLoss\n",
        "        outputs = self.fc(combined_features)\n",
        "\n",
        "        # It is important not to apply softmax here because CrossEntropyLoss does it internally\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "irRLKblISQbQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1EpMezFX45P",
        "outputId": "55b743af-eb91-4426-d1d0-09d149aa2fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: Best Validation Loss: 0.4161\n",
            "Fold 1: Best Validation Loss: 0.3901\n",
            "Fold 2: Best Validation Loss: 0.3943\n",
            "Fold 3: Best Validation Loss: 0.4132\n",
            "Fold 4: Best Validation Loss: 0.4093\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def train_and_evaluate(train_dataset, num_folds=5, num_epochs=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    kf = KFold(n_splits=num_folds)\n",
        "    best_models = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        val_subset = Subset(train_dataset, val_idx)\n",
        "        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        dev_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
        "\n",
        "        model = SimpleFusionClassifier(text_dim=768, image_dim=1000, hidden_dim=128, num_classes=2).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for text_features, image_features, labels in train_loader:\n",
        "                text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(text_features, image_features)\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            dev_loss = evaluate_model(model, dev_loader, device)\n",
        "            if dev_loss < best_val_loss:\n",
        "                best_val_loss = dev_loss\n",
        "                best_model_state = model\n",
        "\n",
        "\n",
        "\n",
        "        best_models.append(best_model_state)\n",
        "        torch.save(best_model_state, f'best_model_fold_{fold}.pth')\n",
        "        print(f\"Fold {fold}: Best Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return best_models\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for text_features, image_features, labels in loader:\n",
        "            text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "            outputs = model(text_features, image_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "simpel_model=train_and_evaluate(train_dataset_bert_restnet,num_folds=5, num_epochs=10)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd5ltXgQRLgS"
      },
      "source": [
        "### **Multi Layer Perzeptron**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, hidden_dim, num_classes):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        # Define linear layers to process text and image inputs\n",
        "        self.text_model = nn.Linear(text_dim, hidden_dim)\n",
        "        self.image_model = nn.Linear(image_dim, hidden_dim)\n",
        "\n",
        "        # Define MLP layers to process combined features\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim * 4)  # First MLP layer\n",
        "        self.fc2 = nn.Linear(hidden_dim * 4, hidden_dim * 2)  # Second MLP layer\n",
        "        self.fc3 = nn.Linear(hidden_dim * 2, num_classes)     # Output layer\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "        # Process text and image features through linear layers\n",
        "        text_features = self.text_model(text_features)\n",
        "        image_features = self.image_model(image_features)\n",
        "\n",
        "        # Combine the features from both modalities\n",
        "        combined_features = torch.cat([text_features, image_features], dim=1)\n",
        "\n",
        "        # Process combined features through MLP layers\n",
        "        combined_features = F.relu(self.fc1(combined_features))\n",
        "        combined_features = F.relu(self.fc2(combined_features))\n",
        "        output = self.fc3(combined_features)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "TYKQDTJ3Th5M"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BOFsiwAeeVl",
        "outputId": "3b085ca7-4fbd-4b09-c5b8-4d8b00368ca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: Best Validation Loss: 0.4121\n",
            "Fold 1: Best Validation Loss: 0.3871\n",
            "Fold 2: Best Validation Loss: 0.3915\n",
            "Fold 3: Best Validation Loss: 0.4135\n",
            "Fold 4: Best Validation Loss: 0.4082\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def train_and_evaluate(train_dataset, num_folds=5, num_epochs=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    kf = KFold(n_splits=num_folds)\n",
        "    best_models = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        val_subset = Subset(train_dataset, val_idx)\n",
        "        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        dev_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
        "\n",
        "        model = MLPClassifier(text_dim=768, image_dim=1000, hidden_dim=128, num_classes=2).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for text_features, image_features, labels in train_loader:\n",
        "                text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(text_features, image_features)\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            dev_loss = evaluate_model(model, dev_loader, device)\n",
        "            if dev_loss < best_val_loss:\n",
        "                best_val_loss = dev_loss\n",
        "                best_model_state = copy.deepcopy(model)\n",
        "\n",
        "        best_models.append(best_model_state)\n",
        "        torch.save(best_model_state, f'best_model_fold_{fold}.pth')\n",
        "        print(f\"Fold {fold}: Best Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return best_models\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for text_features, image_features, labels in loader:\n",
        "            text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "            outputs = model(text_features, image_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "best_models_mlp=train_and_evaluate(train_dataset_bert_restnet,num_folds=5, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUVTLljEWW0s"
      },
      "source": [
        "## Logits and Majority voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YB3pgWE6uSSv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def get_logits_from_models(models, test_loader):\n",
        "    all_model_logits = []  # Diese Liste speichert die Logits von jedem Modell\n",
        "\n",
        "    for model in models:\n",
        "        model.eval()  # Setzt das Modell in den Evaluierungsmodus\n",
        "        model_logits = []  # Eine Liste, um Logits für das aktuelle Modell zu speichern\n",
        "\n",
        "        with torch.no_grad():  # Deaktiviert die Gradientenberechnung\n",
        "            for text_feature, image_feature, _ in test_loader:\n",
        "                # Erhalte die Logits für die aktuellen Features\n",
        "                logits = model(text_feature, image_feature)\n",
        "\n",
        "                model_logits.append(logits)  # Füge die Logits zur Liste hinzu\n",
        "\n",
        "\n",
        "\n",
        "        model_logits_tensor = torch.cat(model_logits , dim=0)\n",
        "        all_model_logits.append(model_logits_tensor)\n",
        "\n",
        "    return all_model_logits  # Gibt eine Liste von Tensoren zurück, jeweils ein Tensor pro Modell\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_label_from_logits(logits):\n",
        "    \"\"\"Convert averaged logits to labels.\"\"\"\n",
        "    return torch.argmax(logits, dim=1)\n",
        "\n",
        "def average_logits(logits_list):\n",
        "    \"\"\"Average logits across different models.\"\"\"\n",
        "    stacked_logits = torch.stack(logits_list, dim=0)\n",
        "    return torch.mean(stacked_logits, dim=0)\n",
        "\n",
        "def majority_vote(labels_list):\n",
        "    \"\"\"Perform a majority vote across different architectures.\"\"\"\n",
        "    # Assuming labels_list is a list of tensors, one per architecture\n",
        "    labels_array = torch.stack(labels_list, dim=0)\n",
        "    labels_mode, _ = torch.mode(labels_array, dim=0)  # Get the mode along the first dim\n",
        "    return labels_mode\n",
        "\n",
        "\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    correct = (predictions == labels).sum().item()\n",
        "    total = len(labels)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxT3_rPCWvBN"
      },
      "source": [
        "## Ensemble evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2G6BHMVd1ON",
        "outputId": "ec8fb2a9-20bc-4a19-b9d6-18c5f5ebd036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8873826903023984\n"
          ]
        }
      ],
      "source": [
        "\n",
        "models=[best_models_rrn,simpel_model,best_models_lstm,best_models_mlp,best_models_crn]\n",
        "\n",
        "\n",
        "all_logits=[]\n",
        "\n",
        "for model in models:\n",
        "  all_logits.append(get_logits_from_models(model, test_loader))\n",
        "\n",
        "\n",
        "\n",
        "final_labels_for_each_arch=[]\n",
        "\n",
        "for logits_list in all_logits:\n",
        "    get_label=get_label_from_logits(average_logits(logits_list))\n",
        "    final_labels_for_each_arch.append(get_label)\n",
        "\n",
        "final_vote_labels = majority_vote(final_labels_for_each_arch)\n",
        "\n",
        "print(calculate_accuracy(final_vote_labels,test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-DlGItzeM3f",
        "outputId": "92e04e67-55a4-465b-9308-224079dbbd49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "macro F1 Score:  0.47016574585635357\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "score = f1_score(test_labels, final_vote_labels, average='macro')\n",
        "print(\"macro F1 Score: \", score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUuCmx5dXRz0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "t6Ba7Vpc_C2m",
        "9t_GnV9ZGY1s",
        "ERsth2odfHgN",
        "Pdw0A_d7f8w2",
        "imn54lR3gELd",
        "39vgMIINDLo8",
        "Vd5ltXgQRLgS"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}