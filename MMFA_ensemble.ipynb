{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsmeeeeeee/MML/blob/main/MMFA_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxtVTjH4DCCV"
      },
      "source": [
        "\n",
        "# Ensemble Method on Multimodal Fusion with Self-Attention (MMFA) for Multimodal Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAAp9bSj3GIb"
      },
      "source": [
        "Group: 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OtrWWSWN4kbD"
      },
      "outputs": [],
      "source": [
        "! pip install torch torchvision\n",
        "! pip install transformers pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIBVh3Ie3j3k"
      },
      "outputs": [],
      "source": [
        "from torchvision import models, transforms\n",
        "from PIL import Image, ImageFile\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhuU0bog5CxN",
        "outputId": "2b977741-e028-480b-a0c7-2b982987192c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQYwsMC1aP-E"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tmHlbvkD--p"
      },
      "source": [
        "**Data extraction is made with Resnet and Bert by Features_extraction file:** https://colab.research.google.com/drive/144qkv0HiAqRXuwlfGNi9M-DxNGmBffYV?usp=drive_link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Ba7Vpc_C2m"
      },
      "source": [
        "### **Read the data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pg6G4tnb70_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Load labels from CSV\n",
        "file_path_labels = \"/content/drive/MyDrive/MultimodalNLP/projekt/data/labels.csv\"\n",
        "labeled_data = pd.read_csv(file_path_labels)\n",
        "labels = labeled_data['overall_sentiment'].values\n",
        "\n",
        "# Define your existing mapping\n",
        "label_mapping = {\n",
        "    \"very_negative\": 0,\n",
        "    \"negative\": 0,\n",
        "    \"positive\": 1,\n",
        "    \"very_positive\": 1\n",
        "}\n",
        "\n",
        "# Get valid indices for labels that exist in the mapping\n",
        "valid_indices = [i for i, label in enumerate(labels) if label in label_mapping]\n",
        "filtered_labels = [labels[i] for i in valid_indices]\n",
        "\n",
        "# Apply the mapping to convert filtered text labels to numeric labels\n",
        "numeric_labels = np.array([label_mapping[label] for label in filtered_labels])\n",
        "\n",
        "# Convert labels to a torch tensor\n",
        "numeric_labels = torch.tensor(numeric_labels, dtype=torch.long)\n",
        "print(\"numeric labels:\", numeric_labels.shape)\n",
        "\n",
        "# Load image features\n",
        "file_path_im = \"/content/drive/MyDrive/MultimodalNLP/projekt/features_data/image_features_restnet.npy\"\n",
        "image_features = np.load(file_path_im)\n",
        "\n",
        "# Load text features\n",
        "file_path_emb = \"/content/drive/MyDrive/MultimodalNLP/projekt/features_data/text_features_bert.npy\"\n",
        "text_features = np.load(file_path_emb)\n",
        "\n",
        "# Filter image and text features using valid indices\n",
        "filtered_image_features = torch.tensor(image_features[valid_indices], dtype=torch.float32)\n",
        "filtered_text_features = torch.tensor(text_features[valid_indices], dtype=torch.float32)\n",
        "\n",
        "print(\"Filtered Image features:\", filtered_image_features.shape)\n",
        "print(\"Filtered Text features:\", filtered_text_features.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_r0i9Tc2mTA",
        "outputId": "a4e53791-7a70-4b87-95f0-1d6b3b450b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numeric labels: torch.Size([4791])\n",
            "Filtered Image features: torch.Size([4791, 1000])\n",
            "Filtered Text features: torch.Size([4791, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t_GnV9ZGY1s"
      },
      "source": [
        "### **Split the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j6BG18LtFfRJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train_text_bert, test_text_bert, train_image_restnet, test_image_restnet, train_labels, test_labels = train_test_split(filtered_text_features,\n",
        "    filtered_image_features, numeric_labels, test_size=0.2, random_state=42)  #20%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DHDjANnymThP",
        "outputId": "b4d62d2e-51ec-420d-9fde-87e9477946cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3832"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(train_text_bert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQeLJ7d1fX-j"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHDO1j7wjqF-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert in Tendors\n",
        "\n",
        "\n",
        "train_dataset_bert_restnet = TensorDataset(train_text_bert, train_image_restnet, train_labels)\n",
        "\n",
        "test_dataset_bert_restnet = TensorDataset(test_text_bert, test_image_restnet, test_labels)\n",
        "\n",
        "test_loader = DataLoader(test_dataset_bert_restnet, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbNKCkegDy6O"
      },
      "source": [
        "\n",
        "\n",
        "## **Build the Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDyRZXttfEfy"
      },
      "source": [
        "####**Self-Attention Block**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBbwYsqb8Zlp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Implementing a self-attention module based on the concept explained in:\n",
        "https://medium.com/@wangdk93/implement-self-attention-and-cross-attention-in-pytorch-1f1a366c9d4b#d075\n",
        "\n",
        "\"\"\"\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, feature_dim, dropout=0.1):\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        # Scaling factor to normalize the dot products\n",
        "        self.scale = 1.0 / (feature_dim ** 0.5)\n",
        "\n",
        "       # Linear transformations for the query, key, and value vectors\n",
        "        self.query = nn.Linear(feature_dim, feature_dim)\n",
        "        self.key = nn.Linear(feature_dim, feature_dim)\n",
        "        self.value = nn.Linear(feature_dim, feature_dim)\n",
        "        #dropout\n",
        "        #self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Generate query, key, value tensors\n",
        "        queries = self.query(x.unsqueeze(1))\n",
        "        keys = self.key(x.unsqueeze(1))\n",
        "        values = self.value(x.unsqueeze(1))\n",
        "\n",
        "        # Calculate the attention scores and apply softmax\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) * self.scale\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        # dropout\n",
        "        #attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Multiply the attention weights by the values\n",
        "        weighted = torch.bmm(attention_weights, values)\n",
        "        return weighted.squeeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERsth2odfHgN"
      },
      "source": [
        "### **LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94Dh1nWqfjOo"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # For bidirectional LSTM\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden and cell states\n",
        "        # Dimensions: (num_layers * num_directions, batch_size, hidden_size)\n",
        "        batch_size = x.size(0)  # Get the current batch size\n",
        "        num_directions = 2 if self.lstm.bidirectional else 1\n",
        "        h0 = torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers * num_directions, batch_size, self.lstm.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])  # Use the last timestep\n",
        "        return out\n",
        "\n",
        "class MultimodalFusionLSTMClassifier(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, hidden_dim, lstm_hidden_dim, num_classes):\n",
        "        super(MultimodalFusionLSTMClassifier, self).__init__()\n",
        "        self.text_model = nn.Linear(text_dim, hidden_dim)\n",
        "        self.image_model = nn.Linear(image_dim, hidden_dim)\n",
        "\n",
        "        # Self-attention module for refining text and image features by focusing on important elements.\n",
        "        self.text_attention = SelfAttention(hidden_dim)\n",
        "        self.image_attention = SelfAttention(hidden_dim)\n",
        "\n",
        "        # LSTM Classifier\n",
        "        #self.classifier = LSTMClassifier(hidden_dim, lstm_hidden_dim, num_layers=2, num_classes=num_classes)\n",
        "        self.classifier = LSTMClassifier(hidden_dim * 2, lstm_hidden_dim, num_layers=2, num_classes=num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "        # Process text features and give attention\n",
        "        text_features = self.text_model(text_features)\n",
        "        text_features = self.text_attention(text_features)\n",
        "\n",
        "        # Process image features and give attention\n",
        "        image_features = self.image_model(image_features)\n",
        "        image_features = self.image_attention(image_features)\n",
        "\n",
        "        # Combine text and image features\n",
        "        combined_features = torch.cat([text_features, image_features], dim=1)\n",
        "        combined_features = combined_features.unsqueeze(1)\n",
        "\n",
        "        # Apply dropout\n",
        "        #combined_features = self.dropout(combined_features)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(combined_features)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7u9zqu8kp5u",
        "outputId": "8026ea4f-e2b9-41ad-cbe1-9e3139fac34b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0, Epoch 1: Training Loss: 0.4125, Validation Loss: 0.4080\n",
            "Fold 0, Epoch 2: Training Loss: 0.3984, Validation Loss: 0.4087\n",
            "Fold 0, Epoch 3: Training Loss: 0.3976, Validation Loss: 0.4142\n",
            "Fold 0, Epoch 4: Training Loss: 0.3965, Validation Loss: 0.4171\n",
            "Fold 0, Epoch 5: Training Loss: 0.3947, Validation Loss: 0.4442\n",
            "Fold 0, Epoch 6: Training Loss: 0.3954, Validation Loss: 0.4281\n",
            "Fold 0, Epoch 7: Training Loss: 0.3968, Validation Loss: 0.4146\n",
            "Fold 0, Epoch 8: Training Loss: 0.3988, Validation Loss: 0.4180\n",
            "Fold 0, Epoch 9: Training Loss: 0.3934, Validation Loss: 0.4162\n",
            "Fold 0, Epoch 10: Training Loss: 0.3926, Validation Loss: 0.4162\n",
            "Fold 1, Epoch 1: Training Loss: 0.4213, Validation Loss: 0.3822\n",
            "Fold 1, Epoch 2: Training Loss: 0.4056, Validation Loss: 0.3831\n",
            "Fold 1, Epoch 3: Training Loss: 0.4039, Validation Loss: 0.3860\n",
            "Fold 1, Epoch 4: Training Loss: 0.4031, Validation Loss: 0.3822\n",
            "Fold 1, Epoch 5: Training Loss: 0.4025, Validation Loss: 0.3842\n",
            "Fold 1, Epoch 6: Training Loss: 0.4046, Validation Loss: 0.3909\n",
            "Fold 1, Epoch 7: Training Loss: 0.4049, Validation Loss: 0.3949\n",
            "Fold 1, Epoch 8: Training Loss: 0.4041, Validation Loss: 0.3868\n",
            "Fold 1, Epoch 9: Training Loss: 0.3993, Validation Loss: 0.3877\n",
            "Fold 1, Epoch 10: Training Loss: 0.3986, Validation Loss: 0.3891\n",
            "Fold 2, Epoch 1: Training Loss: 0.4219, Validation Loss: 0.4002\n",
            "Fold 2, Epoch 2: Training Loss: 0.4036, Validation Loss: 0.3939\n",
            "Fold 2, Epoch 3: Training Loss: 0.4026, Validation Loss: 0.3917\n",
            "Fold 2, Epoch 4: Training Loss: 0.4021, Validation Loss: 0.3936\n",
            "Fold 2, Epoch 5: Training Loss: 0.4066, Validation Loss: 0.3898\n",
            "Fold 2, Epoch 6: Training Loss: 0.4026, Validation Loss: 0.4029\n",
            "Fold 2, Epoch 7: Training Loss: 0.4026, Validation Loss: 0.3925\n",
            "Fold 2, Epoch 8: Training Loss: 0.4004, Validation Loss: 0.3938\n",
            "Fold 2, Epoch 9: Training Loss: 0.4013, Validation Loss: 0.3990\n",
            "Fold 2, Epoch 10: Training Loss: 0.4013, Validation Loss: 0.3992\n",
            "Fold 3, Epoch 1: Training Loss: 0.4110, Validation Loss: 0.4078\n",
            "Fold 3, Epoch 2: Training Loss: 0.3994, Validation Loss: 0.4093\n",
            "Fold 3, Epoch 3: Training Loss: 0.3976, Validation Loss: 0.4071\n",
            "Fold 3, Epoch 4: Training Loss: 0.3989, Validation Loss: 0.4064\n",
            "Fold 3, Epoch 5: Training Loss: 0.3967, Validation Loss: 0.4067\n",
            "Fold 3, Epoch 6: Training Loss: 0.3934, Validation Loss: 0.4064\n",
            "Fold 3, Epoch 7: Training Loss: 0.3983, Validation Loss: 0.4083\n",
            "Fold 3, Epoch 8: Training Loss: 0.3966, Validation Loss: 0.4130\n",
            "Fold 3, Epoch 9: Training Loss: 0.3943, Validation Loss: 0.4072\n",
            "Fold 3, Epoch 10: Training Loss: 0.3932, Validation Loss: 0.4083\n",
            "Fold 4, Epoch 1: Training Loss: 0.4155, Validation Loss: 0.4172\n",
            "Fold 4, Epoch 2: Training Loss: 0.3996, Validation Loss: 0.4058\n",
            "Fold 4, Epoch 3: Training Loss: 0.4004, Validation Loss: 0.4050\n",
            "Fold 4, Epoch 4: Training Loss: 0.3972, Validation Loss: 0.4048\n",
            "Fold 4, Epoch 5: Training Loss: 0.4034, Validation Loss: 0.4242\n",
            "Fold 4, Epoch 6: Training Loss: 0.3984, Validation Loss: 0.4043\n",
            "Fold 4, Epoch 7: Training Loss: 0.4037, Validation Loss: 0.4036\n",
            "Fold 4, Epoch 8: Training Loss: 0.3990, Validation Loss: 0.4072\n",
            "Fold 4, Epoch 9: Training Loss: 0.3973, Validation Loss: 0.4043\n",
            "Fold 4, Epoch 10: Training Loss: 0.3976, Validation Loss: 0.4040\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    \"\"\"Evaluate the model on given data loader and return the average loss.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for text_features, image_features, labels in loader:\n",
        "            text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "            outputs = model(text_features, image_features)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def train_and_save_best_models_lstm(train_dataset, num_folds=5, num_epochs=10):\n",
        "    kf = KFold(n_splits=num_folds)\n",
        "    best_models = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        val_subset = Subset(train_dataset, val_idx)\n",
        "        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        dev_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
        "\n",
        "        model = MultimodalFusionLSTMClassifier(text_dim=768, image_dim=1000, hidden_dim=128, lstm_hidden_dim=256, num_classes=2).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            train_total_loss=0\n",
        "            for text_features, image_features, labels in train_loader:\n",
        "                text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(text_features, image_features)\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_total_loss += loss.item()\n",
        "\n",
        "            # Calculate average training loss for the epoch\n",
        "            avg_train_loss = train_total_loss / len(train_loader)\n",
        "\n",
        "            # Evaluate on the development set\n",
        "            dev_loss = evaluate_model(model, dev_loader, device)\n",
        "            if dev_loss < best_val_loss:\n",
        "              best_val_loss = dev_loss\n",
        "              best_model_state = model\n",
        "\n",
        "\n",
        "            print(f\"Fold {fold}, Epoch {epoch + 1}: Training Loss: {avg_train_loss:.4f}, Validation Loss: {dev_loss:.4f}\")\n",
        "            best_models.append(best_model_state)\n",
        "            torch.save(best_model_state, f'best_model_fold_{fold}.pth')\n",
        "            #print(f\"Fold {fold}: Best Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return best_models\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "best_models_lstm = train_and_save_best_models_lstm(train_dataset_bert_restnet,num_folds=5, num_epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdw0A_d7f8w2"
      },
      "source": [
        "###**RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFBjYZcDti3T"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Classifier_RNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super(Classifier_RNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # Adjusting for bidirectional RNN\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize the hidden state\n",
        "        # Dimensions: (num_layers * num_directions, batch_size, hidden_size)\n",
        "        batch_size = x.size(0)  # Get the current batch size\n",
        "        num_directions = 2 if self.rnn.bidirectional else 1\n",
        "        h0 = torch.zeros(self.rnn.num_layers * num_directions, batch_size, self.rnn.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])  # Use the last timestep of output\n",
        "        return out\n",
        "\n",
        "class MultimodalFusionRNNClassifier(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, hidden_dim, rnn_hidden_dim, num_classes):\n",
        "        super(MultimodalFusionRNNClassifier, self).__init__()\n",
        "        self.text_model = nn.Linear(text_dim, hidden_dim)\n",
        "        self.image_model = nn.Linear(image_dim, hidden_dim)\n",
        "\n",
        "        # Self-attention module for refining text and image features by focusing on important elements.\n",
        "        self.text_attention = SelfAttention(hidden_dim)\n",
        "        self.image_attention = SelfAttention(hidden_dim)\n",
        "\n",
        "        # RNN Classifier\n",
        "        self.classifier = Classifier_RNN(hidden_dim * 2, hidden_dim, num_layers=2, num_classes=num_classes)\n",
        "\n",
        "        #self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "        text_features = self.text_model(text_features)  # [batch_size, hidden_dim]\n",
        "        image_features = self.image_model(image_features.squeeze(1))  # [batch_size, hidden_dim]\n",
        "\n",
        "\n",
        "        # Process image features and give attention\n",
        "        text_features = self.text_attention(text_features)\n",
        "        image_features = self.image_attention(image_features)\n",
        "\n",
        "        # Combine text and image features\n",
        "        combined_features = torch.cat([text_features, image_features], dim=1)\n",
        "        combined_features = combined_features.unsqueeze(1)\n",
        "\n",
        "        # Apply dropout\n",
        "        #combined_features = self.dropout(combined_features)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(combined_features)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn4j5TGrk7og",
        "outputId": "54b2d8e2-7820-42eb-85fb-0b30b96a813e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0, Epoch 1: Training Loss: 0.4212, Validation Loss: 0.4291\n",
            "Fold 0, Epoch 2: Training Loss: 0.3990, Validation Loss: 0.4122\n",
            "Fold 0, Epoch 3: Training Loss: 0.3984, Validation Loss: 0.4099\n",
            "Fold 0, Epoch 4: Training Loss: 0.3972, Validation Loss: 0.4195\n",
            "Fold 0, Epoch 5: Training Loss: 0.4023, Validation Loss: 0.4145\n",
            "Fold 0, Epoch 6: Training Loss: 0.4012, Validation Loss: 0.4195\n",
            "Fold 0, Epoch 7: Training Loss: 0.3969, Validation Loss: 0.4215\n",
            "Fold 0, Epoch 8: Training Loss: 0.3979, Validation Loss: 0.4114\n",
            "Fold 0, Epoch 9: Training Loss: 0.3968, Validation Loss: 0.4145\n",
            "Fold 0, Epoch 10: Training Loss: 0.3967, Validation Loss: 0.4173\n",
            "Fold 1, Epoch 1: Training Loss: 0.4253, Validation Loss: 0.3837\n",
            "Fold 1, Epoch 2: Training Loss: 0.4041, Validation Loss: 0.3909\n",
            "Fold 1, Epoch 3: Training Loss: 0.4112, Validation Loss: 0.3863\n",
            "Fold 1, Epoch 4: Training Loss: 0.4069, Validation Loss: 0.3914\n",
            "Fold 1, Epoch 5: Training Loss: 0.4030, Validation Loss: 0.3842\n",
            "Fold 1, Epoch 6: Training Loss: 0.3984, Validation Loss: 0.4037\n",
            "Fold 1, Epoch 7: Training Loss: 0.3994, Validation Loss: 0.3937\n",
            "Fold 1, Epoch 8: Training Loss: 0.3997, Validation Loss: 0.3922\n",
            "Fold 1, Epoch 9: Training Loss: 0.3987, Validation Loss: 0.3901\n",
            "Fold 1, Epoch 10: Training Loss: 0.3930, Validation Loss: 0.4155\n",
            "Fold 2, Epoch 1: Training Loss: 0.4168, Validation Loss: 0.3912\n",
            "Fold 2, Epoch 2: Training Loss: 0.4063, Validation Loss: 0.3900\n",
            "Fold 2, Epoch 3: Training Loss: 0.4057, Validation Loss: 0.3986\n",
            "Fold 2, Epoch 4: Training Loss: 0.4015, Validation Loss: 0.3904\n",
            "Fold 2, Epoch 5: Training Loss: 0.4050, Validation Loss: 0.4253\n",
            "Fold 2, Epoch 6: Training Loss: 0.4013, Validation Loss: 0.3916\n",
            "Fold 2, Epoch 7: Training Loss: 0.4032, Validation Loss: 0.4008\n",
            "Fold 2, Epoch 8: Training Loss: 0.3971, Validation Loss: 0.4096\n",
            "Fold 2, Epoch 9: Training Loss: 0.3999, Validation Loss: 0.3998\n",
            "Fold 2, Epoch 10: Training Loss: 0.3962, Validation Loss: 0.4000\n",
            "Fold 3, Epoch 1: Training Loss: 0.4243, Validation Loss: 0.4078\n",
            "Fold 3, Epoch 2: Training Loss: 0.4007, Validation Loss: 0.4078\n",
            "Fold 3, Epoch 3: Training Loss: 0.3998, Validation Loss: 0.4079\n",
            "Fold 3, Epoch 4: Training Loss: 0.3984, Validation Loss: 0.4093\n",
            "Fold 3, Epoch 5: Training Loss: 0.3969, Validation Loss: 0.4072\n",
            "Fold 3, Epoch 6: Training Loss: 0.3995, Validation Loss: 0.4084\n",
            "Fold 3, Epoch 7: Training Loss: 0.3940, Validation Loss: 0.4140\n",
            "Fold 3, Epoch 8: Training Loss: 0.3929, Validation Loss: 0.4177\n",
            "Fold 3, Epoch 9: Training Loss: 0.3882, Validation Loss: 0.4294\n",
            "Fold 3, Epoch 10: Training Loss: 0.3851, Validation Loss: 0.4144\n",
            "Fold 4, Epoch 1: Training Loss: 0.4374, Validation Loss: 0.4148\n",
            "Fold 4, Epoch 2: Training Loss: 0.3998, Validation Loss: 0.4050\n",
            "Fold 4, Epoch 3: Training Loss: 0.3997, Validation Loss: 0.4080\n",
            "Fold 4, Epoch 4: Training Loss: 0.4000, Validation Loss: 0.4046\n",
            "Fold 4, Epoch 5: Training Loss: 0.3951, Validation Loss: 0.4059\n",
            "Fold 4, Epoch 6: Training Loss: 0.3961, Validation Loss: 0.4066\n",
            "Fold 4, Epoch 7: Training Loss: 0.3974, Validation Loss: 0.4074\n",
            "Fold 4, Epoch 8: Training Loss: 0.3956, Validation Loss: 0.4086\n",
            "Fold 4, Epoch 9: Training Loss: 0.3953, Validation Loss: 0.4104\n",
            "Fold 4, Epoch 10: Training Loss: 0.3972, Validation Loss: 0.4325\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    \"\"\"Evaluate the model on given data loader and return the average loss.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for text_features, image_features, labels in loader:\n",
        "            text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "            outputs = model(text_features, image_features)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def train_and_save_best_models_rnn(train_dataset, num_folds=5, num_epochs=10):\n",
        "    kf = KFold(n_splits=num_folds)\n",
        "    best_models = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        val_subset = Subset(train_dataset, val_idx)\n",
        "        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        dev_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
        "\n",
        "        model = MultimodalFusionRNNClassifier(text_dim=768, image_dim=1000, hidden_dim=128, rnn_hidden_dim=128, num_classes=2).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            train_total_loss=0\n",
        "            for text_features, image_features, labels in train_loader:\n",
        "                text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(text_features, image_features)\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_total_loss+=loss.item()\n",
        "            # Calculate average training loss for the epoch\n",
        "            avg_train_loss = train_total_loss / len(train_loader)\n",
        "\n",
        "            # Evaluate on the development set\n",
        "            dev_loss = evaluate_model(model, dev_loader, device)\n",
        "            if dev_loss < best_val_loss:\n",
        "                best_val_loss = dev_loss\n",
        "                best_model_state = copy.deepcopy(model)\n",
        "\n",
        "            print(f\"Fold {fold}, Epoch {epoch + 1}: Training Loss: {avg_train_loss:.4f}, Validation Loss: {dev_loss:.4f}\")\n",
        "\n",
        "            best_models.append(best_model_state)\n",
        "            torch.save(best_model_state, f'best_model_fold_{fold}.pth')\n",
        "            #print(f\"Fold {fold}: Best Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return best_models\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "best_models_rrn = train_and_save_best_models_rnn(train_dataset_bert_restnet,num_folds=5, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imn54lR3gELd"
      },
      "source": [
        "###**CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgeGzLLbEhsA"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, input_channels, num_channels, num_classes, hidden_dim):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        kernel_size = 3\n",
        "        padding = 1\n",
        "        stride = 1\n",
        "        pool_kernel_size = 2\n",
        "        pool_stride = 2\n",
        "\n",
        "        # Calculate the size after each layer\n",
        "        size_after_conv = (hidden_dim + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
        "        size_after_pool = size_after_conv // pool_stride\n",
        "\n",
        "        size_after_conv2 = (size_after_pool + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
        "        final_size = size_after_conv2 // pool_stride\n",
        "\n",
        "        self.conv1 = nn.Conv1d(input_channels, num_channels, kernel_size, padding=padding)\n",
        "        self.conv2 = nn.Conv1d(num_channels, num_channels * 2, kernel_size, padding=padding)\n",
        "        self.pool = nn.MaxPool1d(pool_kernel_size, stride=pool_stride)\n",
        "\n",
        "        # Fully connected layer input size calculation\n",
        "        self.fc_input_size = num_channels * 2 * final_size\n",
        "        self.fc = nn.Linear(self.fc_input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultimodalFusionCNNClassifier(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, hidden_dim, num_classes):\n",
        "        super(MultimodalFusionCNNClassifier, self).__init__()\n",
        "        self.text_model = nn.Linear(text_dim, hidden_dim)\n",
        "        self.image_model = nn.Linear(image_dim, hidden_dim)\n",
        "        self.text_attention = SelfAttention(hidden_dim)\n",
        "        self.image_attention = SelfAttention(hidden_dim)\n",
        "        self.classifier = CNNClassifier(2, hidden_dim * 2, num_classes, hidden_dim)  # Pass hidden_dim\n",
        "        self.hidden_dim = hidden_dim  # Ensure hidden_dim is stored as an instance variable\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "        text_features = self.text_model(text_features)\n",
        "        image_features = self.image_model(image_features)\n",
        "        text_features = self.text_attention(text_features)\n",
        "        image_features = self.image_attention(image_features)\n",
        "        combined_features = torch.cat([text_features, image_features], dim=1)\n",
        "        combined_features = combined_features.view(-1, 2, self.hidden_dim)  # Use self.hidden_dim correctly\n",
        "        output = self.classifier(combined_features)\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvRr_ZVxlEjx",
        "outputId": "54eb3f2a-3fa6-4995-96b8-ea6ece0f15b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0, Epoch 1: Training Loss: 0.4492, Validation Loss: 0.4279\n",
            "Fold 0: Best Validation Loss: 0.4279\n",
            "Fold 0, Epoch 2: Training Loss: 0.3979, Validation Loss: 0.4096\n",
            "Fold 0: Best Validation Loss: 0.4096\n",
            "Fold 0, Epoch 3: Training Loss: 0.4041, Validation Loss: 0.4172\n",
            "Fold 0: Best Validation Loss: 0.4096\n",
            "Fold 0, Epoch 4: Training Loss: 0.3939, Validation Loss: 0.4159\n",
            "Fold 0: Best Validation Loss: 0.4096\n",
            "Fold 0, Epoch 5: Training Loss: 0.3927, Validation Loss: 0.4175\n",
            "Fold 0: Best Validation Loss: 0.4096\n",
            "Fold 0, Epoch 6: Training Loss: 0.3981, Validation Loss: 0.4240\n",
            "Fold 0: Best Validation Loss: 0.4096\n",
            "Fold 0, Epoch 7: Training Loss: 0.3794, Validation Loss: 0.4178\n",
            "Fold 0: Best Validation Loss: 0.4096\n",
            "Fold 0, Epoch 8: Training Loss: 0.3725, Validation Loss: 0.4382\n",
            "Fold 0: Best Validation Loss: 0.4096\n",
            "Fold 0, Epoch 9: Training Loss: 0.3732, Validation Loss: 0.4223\n",
            "Fold 0: Best Validation Loss: 0.4096\n",
            "Fold 0, Epoch 10: Training Loss: 0.3741, Validation Loss: 0.4363\n",
            "Fold 0: Best Validation Loss: 0.4096\n",
            "Fold 1, Epoch 1: Training Loss: 0.4600, Validation Loss: 0.3837\n",
            "Fold 1: Best Validation Loss: 0.3837\n",
            "Fold 1, Epoch 2: Training Loss: 0.4084, Validation Loss: 0.3859\n",
            "Fold 1: Best Validation Loss: 0.3837\n",
            "Fold 1, Epoch 3: Training Loss: 0.4074, Validation Loss: 0.3851\n",
            "Fold 1: Best Validation Loss: 0.3837\n",
            "Fold 1, Epoch 4: Training Loss: 0.4028, Validation Loss: 0.3849\n",
            "Fold 1: Best Validation Loss: 0.3837\n",
            "Fold 1, Epoch 5: Training Loss: 0.3911, Validation Loss: 0.4035\n",
            "Fold 1: Best Validation Loss: 0.3837\n",
            "Fold 1, Epoch 6: Training Loss: 0.3920, Validation Loss: 0.4066\n",
            "Fold 1: Best Validation Loss: 0.3837\n",
            "Fold 1, Epoch 7: Training Loss: 0.3825, Validation Loss: 0.4072\n",
            "Fold 1: Best Validation Loss: 0.3837\n",
            "Fold 1, Epoch 8: Training Loss: 0.3805, Validation Loss: 0.3987\n",
            "Fold 1: Best Validation Loss: 0.3837\n",
            "Fold 1, Epoch 9: Training Loss: 0.3709, Validation Loss: 0.4052\n",
            "Fold 1: Best Validation Loss: 0.3837\n",
            "Fold 1, Epoch 10: Training Loss: 0.3587, Validation Loss: 0.4620\n",
            "Fold 1: Best Validation Loss: 0.3837\n",
            "Fold 2, Epoch 1: Training Loss: 0.4645, Validation Loss: 0.3964\n",
            "Fold 2: Best Validation Loss: 0.3964\n",
            "Fold 2, Epoch 2: Training Loss: 0.4036, Validation Loss: 0.3939\n",
            "Fold 2: Best Validation Loss: 0.3939\n",
            "Fold 2, Epoch 3: Training Loss: 0.4013, Validation Loss: 0.3943\n",
            "Fold 2: Best Validation Loss: 0.3939\n",
            "Fold 2, Epoch 4: Training Loss: 0.4002, Validation Loss: 0.4018\n",
            "Fold 2: Best Validation Loss: 0.3939\n",
            "Fold 2, Epoch 5: Training Loss: 0.3978, Validation Loss: 0.3977\n",
            "Fold 2: Best Validation Loss: 0.3939\n",
            "Fold 2, Epoch 6: Training Loss: 0.3906, Validation Loss: 0.4005\n",
            "Fold 2: Best Validation Loss: 0.3939\n",
            "Fold 2, Epoch 7: Training Loss: 0.3852, Validation Loss: 0.4073\n",
            "Fold 2: Best Validation Loss: 0.3939\n",
            "Fold 2, Epoch 8: Training Loss: 0.3894, Validation Loss: 0.4147\n",
            "Fold 2: Best Validation Loss: 0.3939\n",
            "Fold 2, Epoch 9: Training Loss: 0.3858, Validation Loss: 0.4046\n",
            "Fold 2: Best Validation Loss: 0.3939\n",
            "Fold 2, Epoch 10: Training Loss: 0.3734, Validation Loss: 0.4270\n",
            "Fold 2: Best Validation Loss: 0.3939\n",
            "Fold 3, Epoch 1: Training Loss: 0.4424, Validation Loss: 0.4113\n",
            "Fold 3: Best Validation Loss: 0.4113\n",
            "Fold 3, Epoch 2: Training Loss: 0.4036, Validation Loss: 0.4092\n",
            "Fold 3: Best Validation Loss: 0.4092\n",
            "Fold 3, Epoch 3: Training Loss: 0.3957, Validation Loss: 0.4208\n",
            "Fold 3: Best Validation Loss: 0.4092\n",
            "Fold 3, Epoch 4: Training Loss: 0.3956, Validation Loss: 0.4170\n",
            "Fold 3: Best Validation Loss: 0.4092\n",
            "Fold 3, Epoch 5: Training Loss: 0.4031, Validation Loss: 0.4224\n",
            "Fold 3: Best Validation Loss: 0.4092\n",
            "Fold 3, Epoch 6: Training Loss: 0.3833, Validation Loss: 0.4210\n",
            "Fold 3: Best Validation Loss: 0.4092\n",
            "Fold 3, Epoch 7: Training Loss: 0.3798, Validation Loss: 0.4319\n",
            "Fold 3: Best Validation Loss: 0.4092\n",
            "Fold 3, Epoch 8: Training Loss: 0.3706, Validation Loss: 0.4545\n",
            "Fold 3: Best Validation Loss: 0.4092\n",
            "Fold 3, Epoch 9: Training Loss: 0.3636, Validation Loss: 0.4507\n",
            "Fold 3: Best Validation Loss: 0.4092\n",
            "Fold 3, Epoch 10: Training Loss: 0.3557, Validation Loss: 0.4570\n",
            "Fold 3: Best Validation Loss: 0.4092\n",
            "Fold 4, Epoch 1: Training Loss: 0.4386, Validation Loss: 0.4103\n",
            "Fold 4: Best Validation Loss: 0.4103\n",
            "Fold 4, Epoch 2: Training Loss: 0.4028, Validation Loss: 0.4101\n",
            "Fold 4: Best Validation Loss: 0.4101\n",
            "Fold 4, Epoch 3: Training Loss: 0.4013, Validation Loss: 0.4041\n",
            "Fold 4: Best Validation Loss: 0.4041\n",
            "Fold 4, Epoch 4: Training Loss: 0.4015, Validation Loss: 0.4048\n",
            "Fold 4: Best Validation Loss: 0.4041\n",
            "Fold 4, Epoch 5: Training Loss: 0.3942, Validation Loss: 0.4085\n",
            "Fold 4: Best Validation Loss: 0.4041\n",
            "Fold 4, Epoch 6: Training Loss: 0.3912, Validation Loss: 0.4203\n",
            "Fold 4: Best Validation Loss: 0.4041\n",
            "Fold 4, Epoch 7: Training Loss: 0.3806, Validation Loss: 0.4131\n",
            "Fold 4: Best Validation Loss: 0.4041\n",
            "Fold 4, Epoch 8: Training Loss: 0.3736, Validation Loss: 0.4402\n",
            "Fold 4: Best Validation Loss: 0.4041\n",
            "Fold 4, Epoch 9: Training Loss: 0.3758, Validation Loss: 0.4113\n",
            "Fold 4: Best Validation Loss: 0.4041\n",
            "Fold 4, Epoch 10: Training Loss: 0.3708, Validation Loss: 0.4598\n",
            "Fold 4: Best Validation Loss: 0.4041\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    \"\"\"Evaluate the model on given data loader and return the average loss.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for text_features, image_features, labels in loader:\n",
        "            text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "            outputs = model(text_features, image_features)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def train_and_save_best_models_cnn(train_dataset, num_folds=5, num_epochs=10):\n",
        "    kf = KFold(n_splits=num_folds)\n",
        "    best_models = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        val_subset = Subset(train_dataset, val_idx)\n",
        "        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        dev_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
        "\n",
        "        model = MultimodalFusionCNNClassifier(text_dim=768, image_dim=1000, hidden_dim=128, num_classes=2).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            total_train_loss = 0\n",
        "            for text_features, image_features, labels in train_loader:\n",
        "                text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(text_features, image_features)\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "            # Calculate average training loss for the epoch\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "\n",
        "            # Evaluate on the development set\n",
        "            dev_loss = evaluate_model(model, dev_loader, device)\n",
        "            if dev_loss < best_val_loss:\n",
        "                best_val_loss = dev_loss\n",
        "                best_model_state = copy.deepcopy(model)\n",
        "\n",
        "            print(f\"Fold {fold}, Epoch {epoch + 1}: Training Loss: {avg_train_loss:.4f}, Validation Loss: {dev_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "            best_models.append(best_model_state)\n",
        "            torch.save(best_model_state, f'best_model_fold_{fold}.pth')\n",
        "            print(f\"Fold {fold}: Best Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return best_models\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "best_models_crn = train_and_save_best_models_cnn(train_dataset_bert_restnet,num_folds=5, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39vgMIINDLo8"
      },
      "source": [
        "### **Simple model with softmax**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-JPBZ6wDBOi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleFusionClassifier(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, hidden_dim, num_classes):\n",
        "        super(SimpleFusionClassifier, self).__init__()\n",
        "        self.text_model = nn.Linear(text_dim, hidden_dim)\n",
        "        self.image_model = nn.Linear(image_dim, hidden_dim)\n",
        "        self.text_attention = SelfAttention(hidden_dim)\n",
        "        self.image_attention = SelfAttention(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "        # Verarbeiten der Textfeatures\n",
        "        text_features = self.text_model(text_features)\n",
        "        text_features = self.text_attention(text_features)\n",
        "\n",
        "        # Verarbeiten der Bildfeatures\n",
        "        image_features = self.image_model(image_features)\n",
        "        image_features = self.image_attention(image_features)\n",
        "\n",
        "        # Kombinieren der Features\n",
        "        combined_features = torch.cat([text_features, image_features], dim=1)\n",
        "        output = self.fc(combined_features)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1EpMezFX45P",
        "outputId": "a67d7d6d-7d78-4153-d98c-2ca616de4d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0, Epoch 1: Training Loss: 0.4799, Validation Loss: 0.4119\n",
            "Fold 0, Epoch 2: Training Loss: 0.4008, Validation Loss: 0.4159\n",
            "Fold 0, Epoch 3: Training Loss: 0.4000, Validation Loss: 0.4281\n",
            "Fold 0, Epoch 4: Training Loss: 0.3957, Validation Loss: 0.4200\n",
            "Fold 0, Epoch 5: Training Loss: 0.3981, Validation Loss: 0.4213\n",
            "Fold 0, Epoch 6: Training Loss: 0.3899, Validation Loss: 0.4175\n",
            "Fold 0, Epoch 7: Training Loss: 0.3877, Validation Loss: 0.4213\n",
            "Fold 0, Epoch 8: Training Loss: 0.3860, Validation Loss: 0.4456\n",
            "Fold 0, Epoch 9: Training Loss: 0.4083, Validation Loss: 0.4115\n",
            "Fold 0, Epoch 10: Training Loss: 0.3822, Validation Loss: 0.4232\n",
            "Fold 1, Epoch 1: Training Loss: 0.4440, Validation Loss: 0.3887\n",
            "Fold 1, Epoch 2: Training Loss: 0.4077, Validation Loss: 0.4047\n",
            "Fold 1, Epoch 3: Training Loss: 0.3984, Validation Loss: 0.3973\n",
            "Fold 1, Epoch 4: Training Loss: 0.3999, Validation Loss: 0.4458\n",
            "Fold 1, Epoch 5: Training Loss: 0.4071, Validation Loss: 0.3883\n",
            "Fold 1, Epoch 6: Training Loss: 0.3967, Validation Loss: 0.3998\n",
            "Fold 1, Epoch 7: Training Loss: 0.3901, Validation Loss: 0.3976\n",
            "Fold 1, Epoch 8: Training Loss: 0.3891, Validation Loss: 0.3957\n",
            "Fold 1, Epoch 9: Training Loss: 0.3874, Validation Loss: 0.4092\n",
            "Fold 1, Epoch 10: Training Loss: 0.3875, Validation Loss: 0.4007\n",
            "Fold 2, Epoch 1: Training Loss: 0.4776, Validation Loss: 0.4235\n",
            "Fold 2, Epoch 2: Training Loss: 0.4051, Validation Loss: 0.3963\n",
            "Fold 2, Epoch 3: Training Loss: 0.4002, Validation Loss: 0.4046\n",
            "Fold 2, Epoch 4: Training Loss: 0.4032, Validation Loss: 0.3956\n",
            "Fold 2, Epoch 5: Training Loss: 0.4016, Validation Loss: 0.4244\n",
            "Fold 2, Epoch 6: Training Loss: 0.3968, Validation Loss: 0.4162\n",
            "Fold 2, Epoch 7: Training Loss: 0.3878, Validation Loss: 0.3999\n",
            "Fold 2, Epoch 8: Training Loss: 0.3871, Validation Loss: 0.4206\n",
            "Fold 2, Epoch 9: Training Loss: 0.3951, Validation Loss: 0.3974\n",
            "Fold 2, Epoch 10: Training Loss: 0.3875, Validation Loss: 0.4034\n",
            "Fold 3, Epoch 1: Training Loss: 0.4452, Validation Loss: 0.4209\n",
            "Fold 3, Epoch 2: Training Loss: 0.4011, Validation Loss: 0.4151\n",
            "Fold 3, Epoch 3: Training Loss: 0.4099, Validation Loss: 0.4288\n",
            "Fold 3, Epoch 4: Training Loss: 0.3956, Validation Loss: 0.4381\n",
            "Fold 3, Epoch 5: Training Loss: 0.3907, Validation Loss: 0.4118\n",
            "Fold 3, Epoch 6: Training Loss: 0.3873, Validation Loss: 0.4229\n",
            "Fold 3, Epoch 7: Training Loss: 0.3847, Validation Loss: 0.4432\n",
            "Fold 3, Epoch 8: Training Loss: 0.3875, Validation Loss: 0.4206\n",
            "Fold 3, Epoch 9: Training Loss: 0.3890, Validation Loss: 0.4357\n",
            "Fold 3, Epoch 10: Training Loss: 0.3846, Validation Loss: 0.4269\n",
            "Fold 4, Epoch 1: Training Loss: 0.4684, Validation Loss: 0.4084\n",
            "Fold 4, Epoch 2: Training Loss: 0.3997, Validation Loss: 0.4116\n",
            "Fold 4, Epoch 3: Training Loss: 0.3992, Validation Loss: 0.4068\n",
            "Fold 4, Epoch 4: Training Loss: 0.3947, Validation Loss: 0.4064\n",
            "Fold 4, Epoch 5: Training Loss: 0.3982, Validation Loss: 0.4058\n",
            "Fold 4, Epoch 6: Training Loss: 0.3904, Validation Loss: 0.4097\n",
            "Fold 4, Epoch 7: Training Loss: 0.3877, Validation Loss: 0.4318\n",
            "Fold 4, Epoch 8: Training Loss: 0.3925, Validation Loss: 0.4127\n",
            "Fold 4, Epoch 9: Training Loss: 0.3832, Validation Loss: 0.4161\n",
            "Fold 4, Epoch 10: Training Loss: 0.3881, Validation Loss: 0.4092\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def train_and_evaluate(train_dataset, num_folds=5, num_epochs=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    kf = KFold(n_splits=num_folds)\n",
        "    best_models = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        val_subset = Subset(train_dataset, val_idx)\n",
        "        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        dev_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
        "\n",
        "        model = SimpleFusionClassifier(text_dim=768, image_dim=1000, hidden_dim=128, num_classes=2).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            total_train_loss=0\n",
        "            for text_features, image_features, labels in train_loader:\n",
        "                text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(text_features, image_features)\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_train_loss+=loss.item()\n",
        "\n",
        "            avg_train_loss=total_train_loss/len(train_loader)\n",
        "\n",
        "            dev_loss = evaluate_model(model, dev_loader, device)\n",
        "            if dev_loss < best_val_loss:\n",
        "                best_val_loss = dev_loss\n",
        "                best_model_state = model\n",
        "            print(f\"Fold {fold}, Epoch {epoch + 1}: Training Loss: {avg_train_loss:.4f}, Validation Loss: {dev_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "            best_models.append(best_model_state)\n",
        "            torch.save(best_model_state, f'best_model_fold_{fold}.pth')\n",
        "            #print(f\"Fold {fold}: Best Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return best_models\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for text_features, image_features, labels in loader:\n",
        "            text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "            outputs = model(text_features, image_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "simpel_model=train_and_evaluate(train_dataset_bert_restnet,num_folds=5, num_epochs=10)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk4jayhGRKkf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd5ltXgQRLgS"
      },
      "source": [
        "### **Multi Layer Perzeptron**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWWZF5WoRUeF"
      },
      "outputs": [],
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, hidden_dim, num_classes):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.text_model = nn.Linear(text_dim, hidden_dim)\n",
        "        self.image_model = nn.Linear(image_dim, hidden_dim)\n",
        "        self.text_attention = SelfAttention(hidden_dim)\n",
        "        self.image_attention = SelfAttention(hidden_dim)\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim * 4)  # first MLP-layer\n",
        "        self.fc2 = nn.Linear(hidden_dim * 4, hidden_dim * 2)  # secobd MLP-Layer\n",
        "        self.fc3 = nn.Linear(hidden_dim * 2, num_classes)     # third-layer\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "        text_features = self.text_model(text_features)\n",
        "        text_features = self.text_attention(text_features)\n",
        "        image_features = self.image_model(image_features)\n",
        "        image_features = self.image_attention(image_features)\n",
        "        combined_features = torch.cat([text_features, image_features], dim=1)\n",
        "\n",
        "        combined_features = F.relu(self.fc1(combined_features))\n",
        "        combined_features = F.relu(self.fc2(combined_features))\n",
        "        output = self.fc3(combined_features)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BOFsiwAeeVl",
        "outputId": "4855329f-c5dc-4e73-d491-6504fb0c72c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0, Epoch 1: Training Loss: 0.4293, Validation Loss: 0.4218\n",
            "Fold 0, Epoch 2: Training Loss: 0.4009, Validation Loss: 0.4291\n",
            "Fold 0, Epoch 3: Training Loss: 0.4012, Validation Loss: 0.4205\n",
            "Fold 0, Epoch 4: Training Loss: 0.3985, Validation Loss: 0.4187\n",
            "Fold 0, Epoch 5: Training Loss: 0.3923, Validation Loss: 0.4394\n",
            "Fold 0, Epoch 6: Training Loss: 0.3849, Validation Loss: 0.4211\n",
            "Fold 0, Epoch 7: Training Loss: 0.3890, Validation Loss: 0.4323\n",
            "Fold 0, Epoch 8: Training Loss: 0.3774, Validation Loss: 0.4188\n",
            "Fold 0, Epoch 9: Training Loss: 0.3793, Validation Loss: 0.4230\n",
            "Fold 0, Epoch 10: Training Loss: 0.3814, Validation Loss: 0.4312\n",
            "Fold 1, Epoch 1: Training Loss: 0.4255, Validation Loss: 0.3904\n",
            "Fold 1, Epoch 2: Training Loss: 0.4090, Validation Loss: 0.3857\n",
            "Fold 1, Epoch 3: Training Loss: 0.4095, Validation Loss: 0.3889\n",
            "Fold 1, Epoch 4: Training Loss: 0.3986, Validation Loss: 0.3931\n",
            "Fold 1, Epoch 5: Training Loss: 0.4004, Validation Loss: 0.3989\n",
            "Fold 1, Epoch 6: Training Loss: 0.3875, Validation Loss: 0.4020\n",
            "Fold 1, Epoch 7: Training Loss: 0.3811, Validation Loss: 0.4393\n",
            "Fold 1, Epoch 8: Training Loss: 0.3843, Validation Loss: 0.4281\n",
            "Fold 1, Epoch 9: Training Loss: 0.3754, Validation Loss: 0.4329\n",
            "Fold 1, Epoch 10: Training Loss: 0.3715, Validation Loss: 0.4312\n",
            "Fold 2, Epoch 1: Training Loss: 0.4307, Validation Loss: 0.3991\n",
            "Fold 2, Epoch 2: Training Loss: 0.4020, Validation Loss: 0.4037\n",
            "Fold 2, Epoch 3: Training Loss: 0.4031, Validation Loss: 0.4536\n",
            "Fold 2, Epoch 4: Training Loss: 0.4064, Validation Loss: 0.4101\n",
            "Fold 2, Epoch 5: Training Loss: 0.4023, Validation Loss: 0.4030\n",
            "Fold 2, Epoch 6: Training Loss: 0.3906, Validation Loss: 0.4074\n",
            "Fold 2, Epoch 7: Training Loss: 0.3864, Validation Loss: 0.4041\n",
            "Fold 2, Epoch 8: Training Loss: 0.3763, Validation Loss: 0.4162\n",
            "Fold 2, Epoch 9: Training Loss: 0.3735, Validation Loss: 0.4483\n",
            "Fold 2, Epoch 10: Training Loss: 0.3673, Validation Loss: 0.4416\n",
            "Fold 3, Epoch 1: Training Loss: 0.4222, Validation Loss: 0.4127\n",
            "Fold 3, Epoch 2: Training Loss: 0.3965, Validation Loss: 0.4136\n",
            "Fold 3, Epoch 3: Training Loss: 0.3934, Validation Loss: 0.4197\n",
            "Fold 3, Epoch 4: Training Loss: 0.3944, Validation Loss: 0.4229\n",
            "Fold 3, Epoch 5: Training Loss: 0.3876, Validation Loss: 0.4182\n",
            "Fold 3, Epoch 6: Training Loss: 0.3834, Validation Loss: 0.4531\n",
            "Fold 3, Epoch 7: Training Loss: 0.3848, Validation Loss: 0.4246\n",
            "Fold 3, Epoch 8: Training Loss: 0.3779, Validation Loss: 0.4657\n",
            "Fold 3, Epoch 9: Training Loss: 0.3672, Validation Loss: 0.4249\n",
            "Fold 3, Epoch 10: Training Loss: 0.3614, Validation Loss: 0.4439\n",
            "Fold 4, Epoch 1: Training Loss: 0.4259, Validation Loss: 0.4133\n",
            "Fold 4, Epoch 2: Training Loss: 0.4061, Validation Loss: 0.4157\n",
            "Fold 4, Epoch 3: Training Loss: 0.4022, Validation Loss: 0.4256\n",
            "Fold 4, Epoch 4: Training Loss: 0.3916, Validation Loss: 0.4139\n",
            "Fold 4, Epoch 5: Training Loss: 0.3952, Validation Loss: 0.4164\n",
            "Fold 4, Epoch 6: Training Loss: 0.3896, Validation Loss: 0.4268\n",
            "Fold 4, Epoch 7: Training Loss: 0.3862, Validation Loss: 0.4073\n",
            "Fold 4, Epoch 8: Training Loss: 0.3793, Validation Loss: 0.4174\n",
            "Fold 4, Epoch 9: Training Loss: 0.3756, Validation Loss: 0.4134\n",
            "Fold 4, Epoch 10: Training Loss: 0.3691, Validation Loss: 0.4335\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def train_and_evaluate(train_dataset, num_folds=5, num_epochs=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    kf = KFold(n_splits=num_folds)\n",
        "    best_models = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        val_subset = Subset(train_dataset, val_idx)\n",
        "        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        dev_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
        "\n",
        "        model = MLPClassifier(text_dim=768, image_dim=1000, hidden_dim=128, num_classes=2).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            total_train_loss=0\n",
        "            for text_features, image_features, labels in train_loader:\n",
        "                text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(text_features, image_features)\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_train_loss+=loss.item()\n",
        "\n",
        "            avg_train_loss=total_train_loss / len(train_loader)\n",
        "\n",
        "            dev_loss = evaluate_model(model, dev_loader, device)\n",
        "            if dev_loss < best_val_loss:\n",
        "                best_val_loss = dev_loss\n",
        "                best_model_state = copy.deepcopy(model)\n",
        "\n",
        "            print(f\"Fold {fold}, Epoch {epoch + 1}: Training Loss: {avg_train_loss:.4f}, Validation Loss: {dev_loss:.4f}\")\n",
        "\n",
        "            best_models.append(best_model_state)\n",
        "            torch.save(best_model_state, f'best_model_fold_{fold}.pth')\n",
        "            #print(f\"Fold {fold}: Best Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return best_models\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for text_features, image_features, labels in loader:\n",
        "            text_features, image_features, labels = text_features.to(device), image_features.to(device), labels.to(device)\n",
        "            outputs = model(text_features, image_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "best_models_mlp=train_and_evaluate(train_dataset_bert_restnet,num_folds=5, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUVTLljEWW0s"
      },
      "source": [
        "## Logits and Majority voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB3pgWE6uSSv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def get_logits_from_models(models, test_loader):\n",
        "    all_model_logits = []  # Diese Liste speichert die Logits von jedem Modell\n",
        "\n",
        "    for model in models:\n",
        "        model.eval()  # Setzt das Modell in den Evaluierungsmodus\n",
        "        model_logits = []  # Eine Liste, um Logits für das aktuelle Modell zu speichern\n",
        "\n",
        "        with torch.no_grad():  # Deaktiviert die Gradientenberechnung\n",
        "            for text_feature, image_feature, _ in test_loader:\n",
        "                # Erhalte die Logits für die aktuellen Features\n",
        "                logits = model(text_feature, image_feature)\n",
        "\n",
        "                model_logits.append(logits)  # Füge die Logits zur Liste hinzu\n",
        "\n",
        "\n",
        "\n",
        "        model_logits_tensor = torch.cat(model_logits , dim=0)\n",
        "        all_model_logits.append(model_logits_tensor)\n",
        "\n",
        "    return all_model_logits  # Gibt eine Liste von Tensoren zurück, jeweils ein Tensor pro Modell\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_label_from_logits(logits):\n",
        "    \"\"\"Convert averaged logits to labels.\"\"\"\n",
        "    return torch.argmax(logits, dim=1)\n",
        "\n",
        "def average_logits(logits_list):\n",
        "    \"\"\"Average logits across different models.\"\"\"\n",
        "    stacked_logits = torch.stack(logits_list, dim=0)\n",
        "    return torch.mean(stacked_logits, dim=0)\n",
        "\n",
        "def majority_vote(labels_list):\n",
        "    \"\"\"Perform a majority vote across different architectures.\"\"\"\n",
        "    # Assuming labels_list is a list of tensors, one per architecture\n",
        "    labels_array = torch.stack(labels_list, dim=0)\n",
        "    labels_mode, _ = torch.mode(labels_array, dim=0)  # Get the mode along the first dim\n",
        "    return labels_mode\n",
        "\n",
        "\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    correct = (predictions == labels).sum().item()\n",
        "    total = len(labels)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxT3_rPCWvBN"
      },
      "source": [
        "## Ensemble evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2G6BHMVd1ON",
        "outputId": "93f8508d-fefa-4312-dc22-4d68ceab286c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8873826903023984\n"
          ]
        }
      ],
      "source": [
        "\n",
        "models=[best_models_crn,best_models_rrn,simpel_model,best_models_mlp,best_models_lstm]\n",
        "#models=[best_models_crn,best_models_rr]\n",
        "\n",
        "models=[best_models_lstm,best_models_crn]\n",
        "\n",
        "all_logits=[]\n",
        "\n",
        "for model in models:\n",
        "  all_logits.append(get_logits_from_models(model, test_loader))\n",
        "\n",
        "\n",
        "\n",
        "final_labels_for_each_arch=[]\n",
        "\n",
        "for logits_list in all_logits:\n",
        "    means=average_logits(logits_list)\n",
        "    get_label=get_label_from_logits(means)\n",
        "    #print(get_labels)\n",
        "    final_labels_for_each_arch.append(get_label)\n",
        "\n",
        "final_vote_labels = majority_vote(final_labels_for_each_arch)\n",
        "\n",
        "print(calculate_accuracy(final_vote_labels,test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-DlGItzeM3f",
        "outputId": "a04abac6-d32b-47ba-bbc0-0b21a01f30ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "macro F1 Score:  0.47016574585635357\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "score = f1_score(test_labels, final_vote_labels, average='macro')\n",
        "print(\"macro F1 Score: \", score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUuCmx5dXRz0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}